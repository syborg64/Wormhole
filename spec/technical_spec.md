# Sp√©cification Technique
Comme expliqu√© dans le contexte du projet, Wormhole est une solution de **stockage d√©centralis√© de donn√©es**.   
Cette partie du document propose une rapide explication de ce qu'est la d√©centralisation, et de comment cette m√©thode se compare aux autres.   
Le d√©tail technique des fonctions propos√©es par le projet ainsi que sa stack technique sera ensuite abord√©.

## La d√©centralisation (contexte - d√©finition - utilit√©)
Aujoud'hui, petites comme grandes entreprises ont de grands besoins en terme de stockage de donn√©es :
- **Donn√©es internes**
  - Documents de l'entreprise (cloud interne pour les employ√©s)
  - Donn√©es de travail   
    > Assets pour un studio de jeu vid√©o   
    > Datasets scientifiques pour un laboratoire   
    > Training sets pour studios d'intelligence artificielle   
    > Big Data   
    > ... toute donn√©e servant directement l'entreprise   
  - Donn√©es sensibles
    > Comptes, devis et factures de l'entreprise (donn√©es l√©gales)   
    > Donn√©es en rapport avec un client   
- **Donn√©es utilis√©s par un service logiciel propos√© par l'entreprise**
  > Musiques pour une application comme Spotify/Deezer   
  > Vid√©os pour une application comme Youtube/TikTok   
  > Diverses donn√©es stock√©es pour un service comme OneDrive/Google Drive   

Tous ces usages ne sont que des exemples mais repr√©sentent bien les besoins qu'ont les entreprises correctement implant√©s dans l'√®re informatique.   
**Cependant, ce besoin est vite limit√© par une limite physique.**   
En effet, on ne peut pas concentrer une infinit√© de ressources dans un seul serveur.   
Centraliser la donn√©e sur une seule machine poserait aussi un probl√®me d'int√©grit√© des donn√©es en cas de panne.   

**Tr√®s vite arrive la n√©c√©ssit√© d'augmenter le nombre de machines pour r√©pondre au moins √† certaines des exigences suivantes :**
- Besoin de capacit√© massive de stockage (plus de place)
- Besoin de plus de puissance (servir les donn√©es plus vite)
- Fiabilit√© / Gestion de crise
  - R√©sister sans effort aux pannes mineures
  - Suivre sa politique de PCA/PCI ([Plan de Continuit√© d'activit√© Informatique](https://fr.wikipedia.org/wiki/Plan_de_continuit%C3%A9_d%27activit%C3%A9_(informatique))) en cas d'incidant majeur
- Faciliter l'acc√®s pour tous les sites g√©ographiques de l'entreprise

> [!TIP] Plan de Continuit√© d'Activit√© / Informatique
> La **PCA/PCI** est une pratique courante pour les entreprises d√©pendantes de services informatique.   
> G√©n√©ralement mise en place par la direction informatique ainsi que les coeurs de m√©tiers concern√©s, elle prend la forme d'une proc√©dure claire de r√©action aux incidents graves les plus probables.   
> Wormhole n'√©crit pas ce plan pour l'entreprise, mais dispose des param√®tres n√©c√©ssaire pour respecter des proc√©dures d√©finies √† l'avance.   
> Plus d'informations : [Wikip√©dia - Plan de continuit√© d'activit√© (informatique)](https://fr.wikipedia.org/wiki/Plan_de_continuit%C3%A9_d%27activit%C3%A9_(informatique))

Multiplier le nombre de machines pour un m√™me service s'appelle de la d√©centralisation, par opposition √† la centralisation, restreinte √† une entit√©.   
Face √† ce besoin incontournable, les entreprises ont peu de solutions :
- **Utiliser un fournisseur cloud externe**   
  > C'est la solution la plus simple.   
  > Elle est cependant couteuse et l'entreprise n'est plus souveraine de ses donn√©es.   
  > Cela la rend impossible dans certains cas (donn√©es sensibles, donn√©es utilis√©es par un service logiciel ou besoin sp√©cifique)   
  > *A noter que les services cloud utilisent justement la d√©centralisation pour s√©curiser les donn√©es*
- **Semi-centralisation (manuelle)**   
  > Solution consistant √† garder le plus possible une entit√©e centralis√©e (serveur / salle serveur) principale, et d'en pr√©voir une seconde hors ligne sur laquelle on sauvegarde r√©guli√®rement.   
  > En cas de panne, on connecte la seconde entit√© en remplacement. On l'utilisera aussi pour remettre les donn√©es sur l'entit√© principale une fois celle ci en √©tat de marche.   
  > Cette strat√©gie est plus utilis√©e sur les infrastructures √† √©chelle datacenter. Peu accessible par les entreprises moyennes.   
  > Elle induit aussi une possible interruption de service.
- **D√©centralisation (manuelle)**   
  > **La solution ultime**, r√©pondant √† tous les besoins dont nous avons parl√©.   
  > **Cependant il n'existe pas de moyen universel pour mettre en place cette solution. C'est √† cela que Wormhole r√©pond,** en proposant un outil simple, ouvert et universel.

> [!TIP] Wormhole se veut √™tre le Kubernetes de l'espace disque.

## Notre solution : Wormhole
**Wormhole offre une solution simple et d√©clarative pour la cr√©ation d'infrastructures d√©centralis√©es simples comme avanc√©es.**   
Wormhole cr√©√© un syst√®me de fichiers d√©centralis√© entre toutes les machines ajout√©s au r√©seau.   
Une fois mont√©, ce syst√®me de fichier, int√©gr√© nativement, ne diff√®re pas des autres fichiers de la machine.
> [!NOTE] Pour un utilisateur, il n'y a aucune diff√©rence entre un dossier de fichiers locaux et un dossier Wormhole.   
> Il en va de m√™me les logiciels et les applications, les fichiers se comportant comme des fichiers locaux normaux, aucune adaptation n'est n√©c√©ssaire.

### Pour les entreprises :
Adapt√© aux besoins de grande √©chelle, Wormhole permet de monter en un claquement de doigt une infrastructure puissante :
- **Massive**, lib√©r√©e de la centralisation sur un serveur, la croissance n'a pas de limite.
- **Performante**, tirant parti de toute la puissance mise √† disposition de mani√®re optimis√©e, √©vitant la consomation inutile.
- **S√©curis√©e** contre les pertes de donn√©es (m√™me en cas de panne).
- **Sans interruption de service**, m√™me en cas de panne, m√™me lors de modification du r√©seau.
- **Flexible**, avec modification facile de l'infrastructure sans interruption de service.
- **Native**, sans besoin d'adapter les applications et services d√©j√† pr√©sents.
- **Adapt√©e** √† toutes les √©chelles, du petit r√©seau local d'une startup jusqu'aux grandes infrastructures internationales.

> [!IMPORTANT] La configuration simple, claire et d√©clarative permet d'√©viter l'erreur humaine.   
> Une fois lanc√©, l'exp√©rience sera fluide et fiable pour tous les services.
> Le r√©seau peut √™tre modifi√©, des machines ajout√©es ou retir√©es sans interrompre le service.   
> L'entreprise peut facilement d√©finir sa gestion de s√©curit√© pour la concervation des donn√©es, ainsi que ses [plans de continuit√© d'activit√© informatique](https://fr.wikipedia.org/wiki/Plan_de_continuit%C3%A9_d%27activit%C3%A9_(informatique)) pour r√©sister aux incidents mineurs comme majeurs.
<br>

> [!TIP] Evolutif / Scalable
> La nature adaptive de Wormhole le rend ouvert √† des utilisations vari√©es.   
> **L√©ger**, ne demande pas de configuration minimale puissante.   
> **Optimis√©**, il tirera parti des serveurs les plus capables.   

#### Exemples d'utilisations (User Stories) :

> ‚ûï**Startup / PME dans la cybers√©curit√©**   
> Petite √©quipe, n'a pas de p√¥le DSI pour g√©rer de l'infrastructure.   
> N'utilise pas de cloud externe afin de garder la souverainet√© de ses donn√©es.   
> H√©berge ses donn√©es sur ses quelques (ex. 3) petits serveurs NAS.
> - Souhaite simplifier l'organisation de ses donn√©es (actuellement √©parpill√©es sur les diff√©rents NAS)
> - Souhaite assurer l'int√©grit√© de ses donn√©es en cas de panne
> - N'a pas de temps ni d'√©quipe √† consacrer √† cette gestion des donn√©es (organisation, sauvegarde, acc√®s...)
> - Aimerait une solution qui pourra croitre avec l'entreprise
>
> **Solution Wormhole :**
> - Les machines d'un r√©seau sont "fusionn√©es". Pour l'utilisateur final, il n'y a qu'une racine (/) peu importe le nombre de machines individuelles. Libre √† lui de cr√©er les dossiers et l'organisation qu'il souhaite.
> - La configuration d'int√©grit√© est tr√®s compl√®te, elle permet d'anticiper et de r√©agir aux impr√©vus. Voici quelques exemples :
>   - L'option de redondance stocke la quantit√© demand√©e de copies d'un m√™me fichier sur plusieurs machines. Plus il y a de copies, moins le risque de perte est important.
>   - Les options gestion de crise ([PCI](https://fr.wikipedia.org/wiki/Plan_de_continuit%C3%A9_d%27activit%C3%A9_(informatique))) permettent pr√©voir la posture √† adopter si trop de machines tombent pour continuer le fonctionnement normal.
> - La cr√©ation d'un r√©seau est faisable rapidement m√™me par un d√©butant, et ne demande pas de gestion une fois en place.
> - La modification d'un r√©seau ne n√©c√©ssite pas sa suppression, il s'√©quilibre automatiquement lors de l'ajout ou du retrait d'une machine.
>   Il est donc facilement portable sur une infrastructure croissante.
<br>
___

> ‚ûï**Laboratoire**   
> Equipe sp√©cialis√©e, a des serveurs et machine puissantes, mais ce n'est pas le coeur de m√©tier.   
> Proc√®de √† des simulations et analyses, g√©n√©rant des flux tr√®s importants de donn√©es.   
> N'utilise pas de cloud externe, incompatible avec ses besoins de performance.   
> D√©tient des machines puissantes mais sp√©cialis√©es (Ordinateurs pour simulation GPU, Ordinateurs pour analyse CPU, serveurs de stockage massifs).
> - A de grands besoins de performances.
> - Souhaiterait que plusieurs machines distinctes puissent analyser un m√™me set de donn√©es.
> - Les donn√©es sont g√©n√©r√©es, analys√©es et supprim√©es au jour le jour, la perte en cas de panne n'est pas un probl√®me.
> - A des besoins tr√®s changeants (oscille r√©guli√®rement entre quelques Go et quelques dixaines de To) et aimerait pouvoir allouer ses ressources au jour le jour.
>
> **Solution Wormhole :**
> - Stocke intelligemment les donn√©es l√† o√π elles sont le plus demand√©es. Propose un syst√®me de cache pour acc√©l√©rer le syst√®me.
> - Chaque machine du r√©seau a en effet le m√™me set de donn√©es.
> - La configuration permet totalement d'optimiser le r√©seau pour la vitesse et non pour l'int√©grit√© au long terme.
> - La rapidit√© et simplicit√© de mise en place d'un r√©seau permet totalement de monter, utiliser et supprimer un r√©seau pour une seule utilisation.
>   De plus, il suffit de garder le fichier de configuration sous la main pour recr√©er le r√©seau en une commande.
<br>
___

> ‚ûï**Service web**   
> Entreprise r√©cente venant d'exploser ! Ce nouveau r√©seau social permet de partager non pas des photos mais des scans 3D !
> Le r√©seau est atypique mais poss√®de d√©j√† 10.000 utilisateurs r√©guliers ! Stocker tous ces posts p√®se lourd !
> - A un besoin grandissant de place.
> - A un besoin contrast√© de performance. Les ressources devraient √™tres prioris√©es pour les posts en tendances plut√¥t que les posts anciens et rarement vus.
> - A besoin d'un service ininterrompu m√™me en cas de panne.
> - A des exigences d'int√©grit√© autour du minimum l√©gal (autour de 3 copies)
>
> **Solution Wormhole :**
> - Utilise toutes les ressources qui lui sont offertes, et en permet un ajout facile.
> - La configuration des syst√®mes de cache et d'affinit√©s permet de distinguer les serveurs rapides (SSD) et massifs (HDD) et d'utiliser au mieux leur potentiel.
> - Le r√©seau maintenant install√© sur une telle quantit√© de serveurs, la redondance et l'√©quilibrage automatique rendent une interruption de service ou une perte de donn√©es virtuellement impossibles.

<br>
Une fois le syst√®me mis en place, tout fonctionne automatiquement, garantissant une utilisation simple et sans accroc.   
La configuration par fichier est r√©utilisable et partageable. Sa claret√© la rend facile √† comprendre et maintenir m√™me des ann√©es apr√®s sa mise en place.
La plasticit√© du r√©seau le rend fiable, adaptable et modifiable sans mesures compliqu√©es.

### Pour les particuliers
La nature **flexible** de Wormhole lui permet un usage pratique m√™me chez les particuliers.   
Marre de chercher vos documents, photos et projets entre votre NAS, votre ordinateur fixe et votre ordinateur portable?   
Montez en quelques minutes un r√©seau Wormhole, et vos diff√©rents appareils ne font plus qu'un. Vos donn√©es sont disponibles sur tous comme si elles y √©taient !   
> [!IMPORTANT] Une fois install√©, on oublie tr√®s vite la pr√©sence de Wormhole.   
> Et pourtant, l'enfer de chercher ses donn√©es sur diff√©rents appareils, les synchroniser ou les sauvegarder est maintenant de l'histoire ancienne.   
> Wormhole fait tout pour vous üòé   
> On vous a vol√© votre pc portable ? **Vous n'avez pas perdu vos donn√©es.**   
> Votre NAS d√©raille ? **Vous n'avez pas perdu vos donn√©es.**   
> Votre ordinateur fixe brule ?! **Vous n'avez pas perdu vos donn√©es !**   
> Vous avez un nouvel appareil ? **Une commande, et tout est g√©r√©.**

___

## specification

### Interface native

Pour une interaction avec le r√©seau de mani√®re instinctive, l‚Äôacc√®s aux donn√©es se fait par l‚Äôinterface d‚Äôun dossier virtuel mont√© par wormhole. Cela permet de garder les m√™mes moyens d‚Äôinteraction avec les donn√©es que avec tout autre syst√®me de fichier. Ces dossiers virtuels sont permis par les technologies natives telles que FUSE (Linux) ou WinFSP (Windows).

### Int√©gration Universelle

Une des priorit√©s de Wormhole est de rendre le r√©seau accessible par le plus d‚Äôappareils possible afin que le disque virtuel puisse √™tre compatible avec un maximum de m√©thodes de travail. 
Nos objectifs prioritaires pour l‚ÄôEIP sont une int√©gration sur les plateformes suivantes :
- Linux
- Windows
- Mac
Fuse supportant aussi Android fait d‚Äôandroid une plateforme secondaire int√©ressante √† impl√©menter.

Pour simplifier l‚Äôacc√®s aux plateformes non support√©es nativement, une image Docker sera d√©velopp√©e.
Cette image sera propos√©e avec une configuration Kubernetes pour faciliter notre entr√©e dans le monde existant de l‚Äôinformatique distribu√©e.


### Configuration

Notre projet veut allier rapidit√© de mise en place et extensibilit√© de configuration.
Pour r√©pondre √† ces objectifs, nous optons pour la configuration par fichiers. Cette m√©thode a d√©j√† fait ses preuves pour des services comme Docker et Kubernetes, en permettant le partage, la r√©utilisation et le versionning. 
Nous pensons utiliser le format TOML, alliant clart√© et modernit√©, et bien int√©gr√© dans l'environnement Rust.

La configuration se veut la plus compl√®te possible pour moduler tous les aspects du r√©seau. Elle serait donc √† plusieurs niveaux :
Niveau du r√©seau pour le comportement g√©n√©ral.
Niveau Pod avec les informations locales et les affinit√©s propres au pod
Niveau par fichier pour sp√©cifier des exceptions dans leur comportement.

Voici une liste d‚Äôexemples de champs de configurations qui seraient mis √† disposition de l‚Äôutilisateur.
Cette liste n‚Äôest pas exhaustive ou d√©finitive. Notre objectif est de permettre de configurer tout ce qui peut l‚Äô√™tre, ce qui explique que la majorit√© des champs de configuration sp√©cifiques seront d√©finis au cours du projet.

Configuration g√©n√©rale :
Nom unique du r√©seau
Nombre de redondances par fichier
Strat√©gie d‚Äôajout (accepter les nouvelles nodes)
Taille maximale du stockage propos√©
Administration (qui peut modifier la configuration g√©n√©rale)
Strat√©gie de panne
Si elle n‚Äôentrave pas le fonctionnement ou l‚Äôint√©grit√©
Si elle entrave l‚Äôint√©grit√© (manque de redondances, mais aucun fichier perdu)
Si elle entrave le fonctionnement (fichiers manquants)

Configuration par Pod :
Limite d‚Äôespace de stockage
Cache local (propension √† garder des copies locales pour acc√©l√©rer l‚Äôusage)
Affinit√©s (prioriser ou √©viter un pod pour une t√¢che)
Stockage des redondances
Stockage des nouveaux fichiers
Stockage des fichiers les plus demand√©s
Stockage des fichiers les moins demand√©s
Strat√©gie de panne locale (r√©action si d√©connect√© du r√©seau)

Configuration par fichier :
Conserver (force ce Pod √† conserver une version locale)
Ne pas mettre en cache
Lecture seule
Nombre de redondances


Beaucoup d‚Äôoptions de configuration sont ouvertes √† l‚Äôutilisateur . Pour simplifier leurs d√©finition on a choisi de suivre la m√™me m√©thode que docker et kubernetes avec des configurations par fichiers. Plus pr√©cis√©ment sous le format TOML pour sa modernit√© et son int√©gration dans l'√©cosyst√®me rust.

La configuration serait √† plusieurs niveaux, au niveau du r√©seau pour les configuration g√©n√©rale. Au niveau de chaque machine avec les informations locales et les affinit√©s propres au pod et enfin des configuration par fichier pour sp√©cifier des exceptions dans leur comportement.

Distribution de donn√©es

Avec Wormhole, lors de la lecture d‚Äôun fichier qui n‚Äôest pas pr√©sent localement sur la machine, les donn√©es seront t√©l√©charg√©es de la machine h√¥te √† la vol√©e. Cela offre plusieurs possibilit√©es :
Agir √† distance sur le fichier pendant tout le processus (streaming).
Cr√©er une copie locale du fichier pendant son usage, avant d‚Äôexporter les mises √† jour sur le r√©seau.
Agir √† distance est plus lent (latence) et utilise de la bande passante, mais poss√®de le b√©n√©fice de ne pas utiliser d‚Äôespace disque.
Utiliser une copie locale utilise le b√©n√©fice, mais permet une performance accrue.
L‚Äôextensibilit√© de la configuration permet √† l‚Äôutilisateur de param√©trer ce comportement (et d‚Äôautres comportements similaires).
Il est aussi important de noter que de mani√®re automatique, Wormhole stockera les fichiers sur les nodes le demandant souvent, optimisant ainsi le syst√®me entier.

Avec wormhole, √† la lecture d‚Äôun fichier qui n‚Äôest pas pr√©sent sur la machine, les donn√©es seront t√©l√©charg√©es de la machine h√¥te. Ici vient une possibilit√© soit directement stream le contenu du fichier, soit de l'enregistrer avant de transmettre le contenu. L‚Äôune des options consomme plus en network et l‚Äôautre en espace disque. Cet √©quilibre peut √™tre choisi par l‚Äôutilisateur, entre tout stream, tout enregistrer ou bien d√©finir un entre deux en fonction de la fr√©quence de lecture et/ou de la taille du fichier.

Strat√©gies de gestion (tol√©rance de panne, redondance et int√©grit√©, performance‚Ä¶)

La gestion des donn√©es est une question complexe, et elle l‚Äôest encore plus de grandes infrastructures telles que celles que Wormhole peut op√©rer. Ce n‚Äôest pas pour rien que les entreprises ont des √©quipes enti√®res consacr√©es au sujet.

Les exigences pouvant changer du tout au tout selon le cas d‚Äôusage, Wormhole permet de configurer des strat√©gies √† adopter face √† diff√©rents sujets.

Conflits de donn√©es :

La modification simultan√©e d‚Äôun m√™me fichier par plusieurs nodes peut causer des conflits. Il n‚Äôexiste pas de m√©thode de r√©solution de conflits parfaite et universelle. 
L‚Äôutilisateur pourra alors choisir parmi une liste de strat√©gies qui contiendra (sans s‚Äôy limiter) :
Ecraser (garder la version √©crite en dernier)
Garder deux copies


Plusieurs copies d‚Äôun fichiers peut mener √† des conflits lors de modifications simultan√©es donc la r√©solution de conflits sera donc configurable, soit la version la plus r√©cente du fichier sera gard√©e soit une copie avec les anciennes modifications sera gard√©e √† c√¥t√© du fichier original pour permettre √† l‚Äôutilisateur de r√©soudre les conflits sois m√™me.

Int√©grit√© des donn√©es et service ininterrompu (cas g√©n√©ral) :

Il est g√©n√©ralement important d‚Äôassurer l‚Äôint√©grit√© de ses donn√©es en cas de panne. R√©partir des copies des fichiers sur des machines diff√©rentes du r√©seau permet de garantir leur int√©grit√© en cas de d√©faillance.
Non seulement cela, mais cette r√©plication permet au r√©seau de continuer son service sans interruption ou disparition de fichiers, m√™me temporaire.

Ce proc√©d√© porte le nom de redondance a tout de m√™me le d√©faut de consommer un espace disque important.
Selon son usage, l‚Äôutilisateur pourra activer ou non ce proc√©d√© et choisir le nombre de r√©plicas par fichier.
G√©n√©rer un nombre important de copies peut √™tre une op√©ration lourde pour le cluster. L‚Äôutilisateur pourra donc moduler la fr√©quence de mise √† jour des copies.

Int√©grit√© et plan de continuit√© (cas de crise) :

La d√©centralisation et l‚Äôusage de la redondance r√©duisent grandement la probabilit√© d‚Äôincident majeur.
Cependant, Wormhole permet de d√©finir les strat√©gies √† adopter en cas de malfonction g√©n√©ralis√©e.

Les situations sont divis√©es en trois cat√©gories : 
Situation favorable :
Pas de pertes de fichiers, le cluster dispose de l‚Äôespace n√©cessaire pour se r√©√©quilibrer et recr√©er les redondances manquantes.
Abord√© dans la section int√©grit√© des donn√©es et service ininterrompu (cas g√©n√©ral)
Situation mitig√©e :
Pas de pertes de fichiers, mais le cluster manque d‚Äôespace pour s‚Äô√©quilibrer et recr√©er la redondance n√©cessaire.
Situation grave :
Fichiers manquants sur le r√©seau, fonctionnement habituel entrav√©.

Pour chaque situation, l‚Äôutilisateur peut configurer une r√©action appropri√©e.
Exemples de r√©actions (non exhaustif) : 
Ralentir / limiter le trafic
Geler le r√©seau (lecture seule) jusqu‚Äô√† r√©solution du probl√®me ou action de l‚Äôadministrateur
Baisser le nombre de redondances pour augmenter l‚Äôespace libre et poursuivre le service autant que possible
Stopper tout


Un √©l√©ment important dans la sauvegarde de donn√©es est la redondance. R√©partir des copies donn√©es sauvegard√©es sur le r√©seau permet de garantir leur s√©curit√© en cas de probl√®me sur l‚Äôun des disques.
Dans la configuration on pourra l‚Äôactiver et d√©finir le nombre de r√©plications des fichiers, soit au niveau du global soit par dossier/fichiers. 

Plusieurs copies d‚Äôun fichiers peut mener √† des conflits lors de modifications simultan√©es donc la r√©solution de conflits sera donc configurable, soit la version la plus r√©cente du fichier sera gard√©e soit une copie avec les anciennes modifications sera gard√©e √† c√¥t√© du fichier original pour permettre √† l‚Äôutilisateur de r√©soudre les conflits sois m√™me.

Optimisation et r√©partition des charges

La structure d√©centralis√©e en maillage mutualise les capacit√©s et offre de belles perspectives d‚Äôoptimisation de la performance.
Le syst√®me sera capable de g√©rer ‚Äúintelligemment‚Äù son infrastructure, par exemple :
Placer les fichiers et leur redondances sur les nodes les utilisant le plus
Transferts parall√®les (t√©l√©charger diff√©rentes parties d‚Äôun m√™me fichier depuis deux nodes ou plus, doublant la vitesse de transfert. Il en va de m√™me pour l‚Äôupload).
R√©partition des op√©rations lourdes. Exemple : si le nombre de redondances est √©lev√©, chaque node fera le transfert √† seulement deux autres, qui feront de m√™me, etc, √©vitant ainsi √† une seule node de faire tous les transferts.

L‚Äôutilisateur pourra aussi moduler ses besoins pour soulager le r√©seau.
Exemple :
R√©duire la fr√©quence de r√©plication des fichiers, pour √©viter de propager une op√©ration lourde sur le cluster √† chaque √©dition.

La r√©partition en maillage permet de mutualiser les capacit√©s network ce qui ouvre de nombreuses possibilit√©s d‚Äôoptimisation. Par exemple afin d‚Äôoptimiser les transferts de donn√©es. 
Pla√ßant les r√©plications des fichiers les plus utilis√©s sur les nodes avec la meilleure vitesse r√©seau. 
Si un fichier que l‚Äôon t√©l√©charge est pr√©sent sur plusieurs machines, chaque machine peut envoyer une partie du fichier ainsi multipliant largement la vitesse d‚Äôupload. 
Avec un nombre de r√©plication sup√©rieur √† 2, le pod de l‚Äôutilisateur upload une fois sur un pod ‚Äúserveur‚Äù et les pods ‚Äúserveurs‚Äù g√®rent entre eux le reste des r√©plications. Ainsi l‚Äôutilisateur a rapidement sa charge network lib√©r√©e.

Gestion de pod absent 

La connexion au r√©seau √©tant un facteur incertain, il est important de pouvoir r√©agir en cas de d√©connection d‚Äôun pod. D‚Äôun c√¥t√© au niveau du cluster:
R√©√©quilibrer la charge de la r√©plication entre les pods restants
D√©sactiver la lecture des fichiers absent
Et niveau du pod d√©connect√©:
Informer l‚Äôutilisateur
R√©action simple (exemple: freeze)




o - ajout / retrait seamless de nodes (quand ne brise pas l'int√©grit√© des donn√©es)
> wh veut exploiter au maximum la flexibilit√© que permet la d√©centralisation, bla bla

o - pods passifs (portals / clients)

Flexibilit√© et fonctions additionnelles
Le cluster peut √™tre modifi√© sans √™tre interrompu. Cela facilite les √©volutions et permet
L‚Äôajout de nouvelles nodes
Le retrait de nodes
La modification de la configuration

Le cluster s'√©quilibre automatiquement selon le nouveau contexte, sans perturber les services pouvant d√©pendre des donn√©es.

Il est aussi possible de cr√©er des Pods dit ‚ÄúClients‚Äù. Ceux-ci peuvent acc√©der aux fichiers du cluster sans pour autant devenir une maille du r√©seau.
Ils peuvent alors se connecter ou d√©connecter √† la vol√©e sans perturber le syst√®me, ce qui les rend adapt√©s √† un d√©ploiement √† grande √©chelle.
(Par exemple, les ordinateurs portables des collaborateurs de l‚Äôentreprise.)
